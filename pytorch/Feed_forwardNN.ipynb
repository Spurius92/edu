{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a code from [habr blogpost](https://habr.com/ru/company/ods/blog/335998/) by artgor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN:\n",
    "    '''\n",
    "    \n",
    "    The full project includes creating a simple website, collecting hand-written digits and using them\n",
    "    to train a network. \n",
    "    I just needed the code for feed-forward network to understand how it works.\n",
    "    This network has architecture:\n",
    "    input - fully connected layer - ReLU - fully connected layer - softmax - output\n",
    "    \n",
    "    The network has input dimension of N, hidden dimension of H, and performs classification to C classes.\n",
    "    There is a ReLU activation in the first layer, L2 regularization of weight matrices, and loss function is a softmax.   \n",
    "        \n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        '''\n",
    "        Inputs are:\n",
    "        input_size - input dimension D,\n",
    "        hidden_size - hidden dimension H,\n",
    "        output_size - output dimension C.\n",
    "        In this function we initialize the model. First we create a params dictionary which stores paramaters \n",
    "        for our model. Keys in this dictionary are W1, W2, b1, b2: Weights ans biases for the first and second layers \n",
    "        W1 has shape (D, H), W2 - (H, C), b1 - (H,), b2 - (C,).\n",
    "        Weights are initialized using Xavier initialization method. \n",
    "        biases are all zeros at the beginning.\n",
    "        '''\n",
    "        self.W1 = ((2 / input_size) ** 0.5) * np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = ((2 / hidden_size) ** 0.5) * np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "#         self.params = {}\n",
    "#         self.params['W1'] = ((2 / input_size) ** 0.5) * np.random.randn(input_size, hidden_size)\n",
    "#         self.params['b1'] = np.zeros(hidden_size)\n",
    "#         self.params['W2'] = ((2 / hidden_size) ** 0.5) * np.random.randn(hidden_size, output_size)\n",
    "#         self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss(self, X, y, reg):\n",
    "        '''\n",
    "        This fuction is needed to calculate loss and gradients for our two layer neutal network.\n",
    "        X is input data with shape (N, D), where N is number of samples, and D is number of features.\n",
    "        y is the labels matrix.\n",
    "        reg is regularization strenght, if we need it.\n",
    "        \n",
    "        This function returns:\n",
    "        loss: - data loss and regularzation loss for given dataset.\n",
    "        grads - dictionary which maps the parameter names to gradients of those paramaters\n",
    "        with respect to loss function.\n",
    "        '''\n",
    "        W1, b1 = self.W1, self.b1\n",
    "        W2, b2 = self.W2, self.b2\n",
    "        N, D = X.shape\n",
    "        \n",
    "        L1 = np.dot(X, W1) + b1\n",
    "        L1[L1 <= 0] = 0\n",
    "        \n",
    "        L2 = np.dot(L1, W2) + b2\n",
    "        \n",
    "        #softmax function\n",
    "        exp_scores = np.exp(L2 - np.max(L2))\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        scores = L2\n",
    "        \n",
    "        #compute the loss\n",
    "        W1_r = 0.5 * reg * np.sum(W1 * W1)  #i assume that this is a regularized weight matrix \n",
    "        W2_r = 0.5 * reg * np.sum(W2 * W2)\n",
    "        \n",
    "        loss = -np.sum(np.log(probs[range(y.shape[0]), y])) / N + W1_r + W2_r\n",
    "        \n",
    "        #backward pass - compute gradients\n",
    "        grads = {}  \n",
    "        \n",
    "        probs[range(X.shape[0]), y] -= 1  \n",
    "        \n",
    "        dW2 = np.dot(L1.T, probs)   \n",
    "        dW2 /= X.shape[0] # i don't get this part :(\n",
    "        dW2 += reg * W2\n",
    "        grads['W2'] = W2\n",
    "        grads['b2'] = np.sum(probs, axis=0, keepdims=True) / X.shape[0]\n",
    "        \n",
    "        delta = probs.dot(W2.T)\n",
    "        delta = delta * (L1 > 0)\n",
    "        grads['W1'] = np.dot(X.T, delta) / X.shape[0] + reg * W1\n",
    "        grads['b1'] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val, learning_rate=0.01, learning_rate_decay=0.99,\n",
    "              reg=1.0, num_iters=100, batch_size=25, verbose=True):\n",
    "        '''\n",
    "        Function to train the network.\n",
    "        Parameters:\n",
    "        X - training data with dimensions(N, D)\n",
    "        y - training labels with dimensions(N,), y[i] = c means the particular label in the set, \n",
    "                and 0 <= c < C\n",
    "        X_val validation data\n",
    "        y_val - validation labels\n",
    "        learning_rate - scalar giving learning rate for optimization\n",
    "        learning_rate_decay - scalar for slowing down learning rate with each epoch\n",
    "        reg - scalar for regularization strenght\n",
    "        batch size - number of samples to train each step\n",
    "        num_iters - number of steps to take when optimizing\n",
    "        verbose - boolean. If True - print out information about training process\n",
    "        \n",
    "        '''\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "        \n",
    "        # use Stochastic Gradient Descent to optimize parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        \n",
    "        #training cycle\n",
    "        for iter in range(num_iters):\n",
    "            #mini-batch selection\n",
    "            indexes = np.random.choice(X.shape[0], batch_size, replace=True)\n",
    "            \n",
    "            X_batch = X[indexes]\n",
    "            y_batch = y[indexes]\n",
    "            \n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # update weights\n",
    "            self.W1 -= learning_rate * grads['W1']\n",
    "            self.b1 -= learning_rate * grads['b1'][0]\n",
    "            self.W2 -= learning_rate * grads['W2']\n",
    "            self.b2 -= learning_rate * grads['b2'][0]\n",
    "            \n",
    "            if verbose and iter % 100 == 0:\n",
    "                print('Iteration %d / %d: loss %d' % (iter, num_iters, loss))\n",
    "                \n",
    "            # check accuracy every epoch and decay learning_rate\n",
    "            if iter % iterations_per_epoch == 0:\n",
    "                \n",
    "                train_acc = (self.predict(X_train)==y_val).mean()\n",
    "                val_acc = (self.predict(X_val)==y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                \n",
    "                #decay\n",
    "                learning_rate *= learning_rate_decay\n",
    "                \n",
    "        return {\n",
    "            'loss history': loss_history,\n",
    "            'train acc history': train_acc_history,\n",
    "            'val acc history': val_acc_history\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Using weight of our two layer network, make predictions for new data.\n",
    "        Input:\n",
    "        X - array of shape (N, D), where N  - number of samples, D - number of dimensions of the data.\n",
    "        Output:\n",
    "        y_pred - array of shape (N, ), with predicted labels for each data sample. \n",
    "        '''\n",
    "        L1 = np.dot(X, self.W1) + self.b1\n",
    "        L1[L1 <= 0] = 0\n",
    "        L2 = np.dot(L1, self.W2) + self.b2\n",
    "        \n",
    "        exp_scores = np.exp(L2 - np.max(L2))\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        y_preds = np.argmax(probs, axis=1)\n",
    "        \n",
    "        return y_preds\n",
    "    \n",
    "    def predict_single(self, X):\n",
    "        '''\n",
    "        Using weight from our two layer neural network, make prediction for a new sample.\n",
    "        Input:\n",
    "        X - array of shape (N, D), where N is number of samples in a new data, D - number of dimensions(features)\n",
    "        Ouput:\n",
    "        y_pred - array of shape (1,), with predicted label for a single sample.\n",
    "        '''\n",
    "        L1 = np.dot(X, self.W1) + self.b1\n",
    "        L1[L1 <= 0] = 0\n",
    "        \n",
    "        L2 = np.dot(L1, self.W2) + self.b2\n",
    "        exp_scores = np.exp(L2 - np.max(L2))\n",
    "        y_pred = np.argmax(exp_scores)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('C:/Users/Spurius/Desktop/digit-recognizer/train.csv')\n",
    "test = pd.read_csv('C:/Users/Spurius/Desktop/digit-recognizer/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.values[:30000, 1:]\n",
    "y_train = train.values[:30000, :1]\n",
    "X_val = train.values[30000:, 1:]\n",
    "y_val = train.values[30000:, :1]\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 512\n",
    "output_size = 10\n",
    "net = FNN(input_size=784, hidden_size=512, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 / 100: loss 4725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:162: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in less_equal\n",
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\Spurius\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:80: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss history': [4725.82037279748,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'train acc history': [0.09958318055555555],\n",
       " 'val acc history': [0.09958295138888888]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train(X_train, y_train, X_val, y_val, learning_rate=0.01, learning_rate_decay=1, reg=1.0, num_iters=100, batch_size=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FNN object at 0x0000020AEEB40F98>\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-303c8a3278e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "isinstance(net, FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\python36.zip',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\DLLs',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\lib',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\lib\\\\site-packages\\\\html5lib-1.0.1-py3.6.egg',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\lib\\\\site-packages\\\\pip-9.0.3-py3.6.egg',\n",
       " 'C:\\\\Users\\\\Spurius\\\\Anaconda3\\\\envs\\\\idp\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\Spurius\\\\.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
