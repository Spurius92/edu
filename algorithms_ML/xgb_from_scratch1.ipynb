{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retype of [this tutorial](https://habr.com/ru/company/mailru/blog/438560/) by Temkahap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    '''\n",
    "    class RegressionTree решает задачу регрессии. Основан на рекурсивных вызовах, когда прописываются условия выхода из рекурсии\n",
    "    '''\n",
    "    def __init__(self, max_depth=3, n_epoch=10, min_size=8):\n",
    "        self.max_depth = max_depth #максимальная глубина\n",
    "        self.min_size = min_size #минимальный размер поддерева\n",
    "        self.value = 0 #значение в поддереве(среднее по всем листьям)\n",
    "        self.feature_idx = -1 #номер лучшего признака\n",
    "        self.feature_treshold = 0#значение лучшего признака\n",
    "        self.right = None# правый потомок\n",
    "        self.left = None#левый потомок\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Процедура обучения дерева. На выходе получим обученную модель\n",
    "        '''\n",
    "        #инициализируем начальные значения\n",
    "        self.value = y.mean()\n",
    "        base_error = ((y - self.value) ** 2).sum()\n",
    "        error = base_error\n",
    "        flag = 0\n",
    "        \n",
    "        #ошибки в левом и правом поддереве\n",
    "        prev_error_left = base_error\n",
    "        prev_error_right = 0\n",
    "        \n",
    "        #если дошли до глубины 0 - выходим\n",
    "        if self.max_depth <= 1:\n",
    "            return\n",
    "        \n",
    "        dim_shape = X.shape[1]\n",
    "        \n",
    "        #значения в левом и правом поддереве\n",
    "        left_value = 0\n",
    "        right_value = 0\n",
    "        \n",
    "        #начинаем цикл по признакам\n",
    "        for feat in range(dim_shape):\n",
    "            \n",
    "            #сортируем признаки\n",
    "            idxs = np.argsort(X[:, feat])\n",
    "            \n",
    "            #количесвто сэмплов в левом и правом поддереве\n",
    "            N = X.shape[0]\n",
    "            N1, N2 = N, 0\n",
    "            thres = 1\n",
    "            \n",
    "            #начинаем проходиться по значениям признака\n",
    "            while thres < N - 1:\n",
    "                N1 -= 1\n",
    "                N2 += 1\n",
    "                idx = idxs[thres]\n",
    "                x = X[idx, feat]\n",
    "                \n",
    "                #пропускаем одинаковые признаки\n",
    "                if thres < N - 1 and x == X[idxs[thres+1], feat]:\n",
    "                    thres += 1\n",
    "                    continue\n",
    "                    \n",
    "                #данные, которые получаются в результате такого сплита\n",
    "                target_left = y[idxs][thres:]\n",
    "                target_right = y[idxs][:thres]\n",
    "                mean_left = y[idxs][thres:].mean()\n",
    "                mean_right = y[idxs][:thres].mean()\n",
    "                \n",
    "                #на этом шаге уже нужно считать ошибку\n",
    "                #генерируем предикты(среднее в потомках)\n",
    "                left_shape = target_left.shape[0]\n",
    "                right_shape = target_right.shape[0]\n",
    "                mean_left_array = [mean_left for _ in range(left_shape)]\n",
    "                mean_right_array = [mean_right for _ in range(right_shape)]\n",
    "                \n",
    "                #считаем ошибку слева и справа\n",
    "                prev_error_left = N1 / N * mse(target_left, mean_left_array)\n",
    "                prev_error_right = N2 / N * mse(target_right, mean_right_array)\n",
    "                \n",
    "                #если выполняются условия сплита - то обновляем\n",
    "                if (prev_error_left + prev_error_right < error):\n",
    "                    if (min(N1, N2) < self.min_size):\n",
    "                        self.feature_idx = feat\n",
    "                        self.feature_treshold = x\n",
    "                        left_value = mean_left\n",
    "                        right_value = mean_right\n",
    "                        \n",
    "                        flag = 1\n",
    "                        error = prev_error_left + prev_error_right\n",
    "                        \n",
    "                thres += 1\n",
    "                \n",
    "        #если не нашли лучший сплит - выходим\n",
    "        if self.feature_idx == -1:\n",
    "            return\n",
    "        \n",
    "        #если дошли сюда - значит есть хорошее разбиение\n",
    "        #можно обучать дальше \n",
    "        #инициализируем потомков- тоже деревья решений\n",
    "        self.left = RegressionTree(self.max_depth - 1)\n",
    "        self.left.value = left_value\n",
    "        self.right = RegressionTree(self.max_depth - 1)\n",
    "        self.right.value = right_value\n",
    "        \n",
    "        #индексы потомков\n",
    "        idxs_l = (X[:, self.feature_idx] > self.feature_treshold)\n",
    "        idxs_r = (X[:, self.feature_idx] <= self.feature_treshold)\n",
    "        \n",
    "        #обучаем\n",
    "        self.left.fit(X[idxs_l, :], y[idxs_l])\n",
    "        self.right.fit(X[idxs_r, :], y[idxs_r])\n",
    "        \n",
    "    def __predict(self, x):\n",
    "        '''\n",
    "        функция для предсказания. Идем в узлы - ищем потомков и смотрим в конце self.value, которое и будет ответом\n",
    "        '''\n",
    "        if self.feature_idx == -1:\n",
    "            return self.value\n",
    "        \n",
    "        if x[self.feature_idx] > self.feature_treshold:\n",
    "            return self.left.__predict(x)\n",
    "        else:\n",
    "            return self.right.__predict(x)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Просто вызываем __predict для каждой строчки матрицы\n",
    "        '''\n",
    "        y = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = self.__predict(X[i])\n",
    "            return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод fit, как понятно из названия, обучает модель. На вход подаётся обучающая выборка и происходит процедура обучения дерева. Сортируя признаки, мы ищем наилучшее разбиение дерева с точки зрения уменьшения энтропии, в данном случае mse. Определить, что удалось найти хороший сплит, очень просто, достаточно выполнения двух условий. Мы не хотим, чтобы в разбиение попадало мало объектов (защита от переобучения), и средневзвешенная ошибка по mse должна быть меньше той ошибки, которая есть сейчас в дереве — мы ищем тот самый прирост information gain. Пройдя таким образом все признаки и все уникальные значения по ним, мы переберём все варианты и выберем наилучшее разбиение. А дальше делаем рекурсивный вызов на полученных разбиениях до тех пор, пока не выполнятся условия выхода из рекурсии.\n",
    "\n",
    "Метод __predict, как понятно из названия, делает предикт. Получив на вход объект, он идёт по узлам полученного дерева — в каждом узле зафиксирован номер признака и значение по нему, и в зависимости от того, какое значение у поступившего метода объекта по этому признаку, мы идём либо в правого потомка, либо в левого, пока не дойдём до листа, в котором и будет ответ для данного объекта.\n",
    "\n",
    "Метод predict делает то же самое, что и прошлый метод, только для группы объектов.\n",
    "\n",
    "Импортируем всем известный набор данных о домах в Калифорнии. Это обычный датасет с данными и таргетом для решения задачи регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\Spurius\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "X = np.array(data.data)\n",
    "y = np.array(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A = RegressionTree(max_depth=2)\n",
    "A.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 576 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = DecisionTreeRegressor(max_depth=2)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
